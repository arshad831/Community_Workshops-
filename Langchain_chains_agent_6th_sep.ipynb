{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arshad831/Community_Workshops-/blob/main/Langchain_chains_agent_6th_sep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c26845d",
      "metadata": {
        "id": "2c26845d"
      },
      "source": [
        "# Langchain crash course"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai langchain\n"
      ],
      "metadata": {
        "id": "nP6SGg3YukAK"
      },
      "id": "nP6SGg3YukAK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "f31c4cc6",
      "metadata": {
        "id": "f31c4cc6"
      },
      "outputs": [],
      "source": [
        "# from secret_key import openapi_key\n",
        "# import os\n",
        "# os.environ['OPENAI_API_KEY'] = openapi_key\n",
        "\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = \"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\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ed0dc6a",
      "metadata": {
        "id": "9ed0dc6a"
      },
      "source": [
        "## LLMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "fa352d5f",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa352d5f",
        "outputId": "9fc8531d-ff1b-451a-f810-bce2e1fb6509"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\"The Maharaja's Table\"\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature=0.9)\n",
        "name = llm.predict(\"I want to open a restaurant for Indian food. Suggest a fency name for this.\")\n",
        "print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b56e8581",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "b56e8581",
        "outputId": "65626bc8-32da-41d2-f374-ccd545eca6ff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nCurry Palace'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "llm(\"I want to open a restaurant for Indian food. Suggest a fency name for this.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0782a2dd",
      "metadata": {
        "id": "0782a2dd"
      },
      "source": [
        "## Prompt Templates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "7a306b9d",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a306b9d",
        "outputId": "d6402a49-4e5e-475a-dc23-e2573b5c4f29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I want to open a restaurant for Indian food. Suggest a fency name for this.\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables =['cuisine'],\n",
        "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
        ")\n",
        "p = prompt_template_name.format(cuisine=\"Indian\")\n",
        "print(p)\n",
        "\n",
        "# print(llm.predict(p))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af406b92",
      "metadata": {
        "id": "af406b92"
      },
      "source": [
        "## Chains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "ba65c213",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ba65c213",
        "outputId": "fb44be7e-f9f2-4552-fd9e-47e7a7aee0a0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n\"The Maharaja\\'s Palace\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template_name)\n",
        "chain.run(\"Indian\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "e5ccee75",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "e5ccee75",
        "outputId": "93ba523f-f485-4cd7-f049-515554705bf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mI want to open a restaurant for Mexican food. Suggest a fency name for this.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nAmigos Nuevos Cocina Mexicana'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "chain = LLMChain(llm=llm, prompt=prompt_template_name, verbose=True)\n",
        "chain.run(\"Mexican\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "21098937",
      "metadata": {
        "id": "21098937"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(temperature=0.6)\n",
        "\n",
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables =['cuisine'],\n",
        "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
        ")\n",
        "\n",
        "name_chain =LLMChain(llm=llm, prompt=prompt_template_name)\n",
        "\n",
        "prompt_template_items = PromptTemplate(\n",
        "    input_variables = ['restaurant_name'],\n",
        "    template=\"\"\"Suggest some menu items for {restaurant_name}\"\"\"\n",
        ")\n",
        "\n",
        "food_items_chain = LLMChain(llm=llm, prompt=prompt_template_items)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87a98d9f",
      "metadata": {
        "id": "87a98d9f"
      },
      "source": [
        "#### Simple Sequential Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "d9fd9a79",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9fd9a79",
        "outputId": "e2390c70-f20f-461c-861c-979c6078a964"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "- Chicken Tikka Masala\n",
            "- Lamb Rogan Josh\n",
            "- Vegetable Korma\n",
            "- Aloo Gobi\n",
            "- Naan Bread\n",
            "- Raita\n",
            "- Samosas\n",
            "- Chana Masala\n",
            "- Bhindi Masala\n",
            "- Saag Paneer\n",
            "- Tandoori Chicken\n",
            "- Mango Lassi\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import SimpleSequentialChain\n",
        "chain = SimpleSequentialChain(chains = [name_chain, food_items_chain])\n",
        "\n",
        "content = chain.run(\"Indian\")\n",
        "print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0386d05c",
      "metadata": {
        "id": "0386d05c"
      },
      "source": [
        "#### Sequential Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "49dc0fae",
      "metadata": {
        "id": "49dc0fae"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(temperature=0.7)\n",
        "\n",
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables =['cuisine'],\n",
        "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
        ")\n",
        "\n",
        "name_chain =LLMChain(llm=llm, prompt=prompt_template_name, output_key=\"restaurant_name\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "9dea8402",
      "metadata": {
        "id": "9dea8402"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(temperature=0.7)\n",
        "\n",
        "prompt_template_items = PromptTemplate(\n",
        "    input_variables = ['restaurant_name'],\n",
        "    template=\"Suggest some menu items for {restaurant_name}.\"\n",
        ")\n",
        "\n",
        "food_items_chain =LLMChain(llm=llm, prompt=prompt_template_items, output_key=\"menu_items\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "1ec1be10",
      "metadata": {
        "id": "1ec1be10"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import SequentialChain\n",
        "\n",
        "chain = SequentialChain(\n",
        "    chains = [name_chain, food_items_chain],\n",
        "    input_variables = ['cuisine'],\n",
        "    output_variables = ['restaurant_name', \"menu_items\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "4653c540",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4653c540",
        "outputId": "61ffdfb5-3207-4786-baf2-424046bd785c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cuisine': 'French',\n",
              " 'restaurant_name': '\\n\\nLe Gourmet Francais',\n",
              " 'menu_items': '\\n\\n-Tartine au Fromage et Jambon\\n-Quiche Lorraine\\n-Croque Monsieur\\n-Croquettes de Fromage\\n-Salade Nicoise\\n-Risotto aux Champignons\\n-Blanquette de Veau\\n-Tarte Tatin\\n-Pain perdu aux Fruits Rouges\\n-Boeuf Bourguignon\\n-Confit de Canard\\n-Cassoulet\\n-Mousse au Chocolat\\n-Crème Brûlée'}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "chain({\"cuisine\": \"French\"})"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-XMfkLOp2Q5G"
      },
      "id": "-XMfkLOp2Q5G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4069a75e",
      "metadata": {
        "id": "4069a75e"
      },
      "source": [
        "## Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "5e6ee719",
      "metadata": {
        "id": "5e6ee719"
      },
      "outputs": [],
      "source": [
        "# make sure yoy have installed this package: pip install google-search-results\n",
        "# from secret_key import serpapi_key\n",
        "# os.environ['SERPAPI_API_KEY'] = serpapi_key\n",
        "\n",
        "os.environ['SERPAPI_API_KEY'] = \"722250fb6e1ce4c446e0d12ee5d7c6df99bb5320bdc5988ae20823d0f2e8125c\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install google-search-results"
      ],
      "metadata": {
        "id": "eQlykSw36yrx"
      },
      "id": "eQlykSw36yrx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "471b2c6b",
      "metadata": {
        "id": "471b2c6b"
      },
      "source": [
        "#### serpapi and llm-math tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "fec4212d",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "id": "fec4212d",
        "outputId": "16a19923-d76a-4090-9daf-8afd3ae72819"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to find the GDP of US in 2022\n",
            "Action: Search\n",
            "Action Input: US GDP in 2022\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m$25.46 trillion\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I need to add 5 to this number\n",
            "Action: Calculator\n",
            "Action Input: 25.46 + 5\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 30.46\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: The GDP of US in 2022 plus 5 is $30.46 trillion.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The GDP of US in 2022 plus 5 is $30.46 trillion.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "from langchain.agents import AgentType, initialize_agent, load_tools\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "\n",
        "# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\n",
        "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
        "\n",
        "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
        "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
        "\n",
        "# Let's test it out!\n",
        "agent.run(\"What was the GDP of US in 2022 plus 5?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09cd3a12",
      "metadata": {
        "id": "09cd3a12"
      },
      "source": [
        "#### Wikipedia and llm-math tool"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fz2zj_Kw7_46",
        "outputId": "2b16774b-5049-451a-b44f-f7226b9cfbaa"
      },
      "id": "Fz2zj_Kw7_46",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.11.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2023.7.22)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.4.1)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=eaf7102d5e0bd587004087b17a8a4193aecf52e35051366334b7edaf00b4ed13\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "14d06ce6",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "14d06ce6",
        "outputId": "ded2975a-ca7d-4733-bdd4-0438cafce2f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to find out when Elon Musk was born and then calculate his age.\n",
            "Action: Wikipedia\n",
            "Action Input: Elon Musk\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mPage: Elon Musk\n",
            "Summary: Elon Reeve Musk ( EE-lon; born June 28, 1971) is a business magnate and investor. Musk is the founder, chairman, CEO and chief technology officer of SpaceX;  angel investor, CEO, product architect and former chairman of Tesla, Inc.; owner, chairman and CTO of X Corp.; founder of the Boring Company; co-founder of Neuralink and OpenAI; and president of the Musk Foundation. He is the wealthiest person in the world, with an estimated net worth of US$226 billion as of September 2023, according to the Bloomberg Billionaires Index, and $249 billion according to Forbes, primarily from his ownership stakes in both Tesla and SpaceX.Musk was born in Pretoria, South Africa, and briefly attended the University of Pretoria before immigrating to Canada at age 18, acquiring citizenship through his Canadian-born mother. Two years later, he matriculated at Queen's University in Kingston, Ontario. Musk later transferred to the University of Pennsylvania, and received bachelor's degrees in economics and physics there. He moved to California in 1995 to attend Stanford University. However, Musk dropped out after two days and, with his brother Kimbal, co-founded online city guide software company Zip2. The startup was acquired by Compaq for $307 million in 1999, and with $12 million of the money he made, that same year Musk co-founded X.com, a direct bank. X.com merged with Confinity in 2000 to form PayPal. \n",
            "In 2002, eBay acquired PayPal for $1.5 billion, and that same year, with $100 million of the money he made, Musk founded SpaceX, a spaceflight services company. In 2004, he became an early investor in electric vehicle manufacturer Tesla Motors, Inc. (now Tesla, Inc.). He became its chairman and product architect, assuming the position of CEO in 2008. In 2006, Musk helped create SolarCity, a solar energy company that was acquired by Tesla in 2016 and became Tesla Energy. In 2013, he proposed a hyperloop high-speed vactrain transportation system. In 2015, he co-founded OpenAI, a nonprofit artificial intelligence research company. The following year, Musk co-founded Neuralink—a neurotechnology company developing brain–computer interfaces—and the Boring Company, a tunnel construction company. In 2022, he acquired Twitter for $44 billion and subsequently merged the company into newly created X Corp. and rebranded the service as X the following year. In March 2023, he founded xAI, an artificial-intelligence company.\n",
            "Musk has expressed views that have made him a polarizing figure. He has been criticized for making unscientific and misleading statements, including that of spreading COVID-19 misinformation, and promoting conspiracy theories. His Twitter ownership has been similarly controversial, including laying off a large number of employees, an increase in hate speech on the platform and features such as Twitter Blue and the implementation of limits on the amount of viewable Tweets per day being criticized. In 2018, the U.S. Securities and Exchange Commission (SEC) sued him for falsely tweeting that he had secured funding for a private takeover of Tesla. To settle the case, Musk stepped down as the chairman of Tesla and paid a $20 million fine.\n",
            "\n",
            "Page: Acquisition of Twitter by Elon Musk\n",
            "Summary: Business magnate Elon Musk initiated an acquisition of American social media company Twitter, Inc. on April 14, 2022, and concluded it on October 27, 2022. Musk had begun buying shares of the company in January 2022, becoming its largest shareholder by April with a 9.1 percent ownership stake. Twitter invited Musk to join its board of directors, an offer he initially accepted before declining. On April 14, Musk made an unsolicited offer to purchase the company, to which Twitter's board initially responded with a \"poison pill\" strategy to resist a hostile takeover, before unanimously accepting Musk's buyout offer of $44 billion on April 25. Musk stated that he planned to introduce new features to the platform, make its algorithms open-\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I need to calculate Elon Musk's age in 2023.\n",
            "Action: Calculator\n",
            "Action Input: (2023 - 1971)\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 52\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
            "Final Answer: Elon Musk was born in 1971 and is 52 years old in 2023.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Elon Musk was born in 1971 and is 52 years old in 2023.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "# install this package: pip install wikipedia\n",
        "\n",
        "# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\n",
        "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
        "\n",
        "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
        "agent = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Let's test it out!\n",
        "agent.run(\"When was Elon musk born? What is his age right now in 2023?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6be7ee7",
      "metadata": {
        "id": "b6be7ee7"
      },
      "source": [
        "## Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2acab5d0",
      "metadata": {
        "id": "2acab5d0",
        "outputId": "8fd5a257-fa2a-4351-980c-675e212626c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Taco Fiesta\n"
          ]
        }
      ],
      "source": [
        "chain = LLMChain(llm=llm,prompt=prompt_template_name)\n",
        "name = chain.run(\"Mexican\")\n",
        "print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bc200f9",
      "metadata": {
        "id": "5bc200f9",
        "outputId": "d6678c6d-a344-46c5-ad2d-df8552186f69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Maharaja's Palace Cuisine\n"
          ]
        }
      ],
      "source": [
        "name = chain.run(\"Indian\")\n",
        "print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "229a6888",
      "metadata": {
        "id": "229a6888"
      },
      "outputs": [],
      "source": [
        "chain.memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f492fb5a",
      "metadata": {
        "scrolled": true,
        "id": "f492fb5a",
        "outputId": "d0824d66-c49c-4887-865a-134e9d8c018e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "NoneType"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(chain.memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "871492be",
      "metadata": {
        "id": "871492be"
      },
      "source": [
        "#### ConversationBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53eea298",
      "metadata": {
        "id": "53eea298",
        "outputId": "3e2e1f3b-fbcc-4bc5-a0ac-6abd7a64863c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Taco Fiesta\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template_name, memory=memory)\n",
        "name = chain.run(\"Mexican\")\n",
        "print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0de5d50b",
      "metadata": {
        "id": "0de5d50b",
        "outputId": "e87d68a3-a323-4671-ae37-15553a5718aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Al-Fez Restaurant\n"
          ]
        }
      ],
      "source": [
        "name = chain.run(\"Arabic\")\n",
        "print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cc88888",
      "metadata": {
        "scrolled": true,
        "id": "5cc88888",
        "outputId": "fa815ac2-fc37-481d-8501-51ddeb3ec278"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Human: Mexican\n",
            "AI: \n",
            "\n",
            "Taco Fiesta\n",
            "Human: Arabic\n",
            "AI: \n",
            "\n",
            "Al-Fez Restaurant\n"
          ]
        }
      ],
      "source": [
        "print(chain.memory.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0a88b5b",
      "metadata": {
        "id": "a0a88b5b"
      },
      "source": [
        "#### ConversationChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "687ddd2f",
      "metadata": {
        "id": "687ddd2f",
        "outputId": "80bfb842-ad84-40da-b809-ec516fb22c96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "{history}\n",
            "Human: {input}\n",
            "AI:\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "\n",
        "convo = ConversationChain(llm=OpenAI(temperature=0.7))\n",
        "print(convo.prompt.template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47ad5062",
      "metadata": {
        "id": "47ad5062",
        "outputId": "991b8b6e-caea-4aae-f09a-ff686a8ddb19"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' The first Cricket World Cup was won by the West Indies in 1975.'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "convo.run(\"Who won the first cricket world cup?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03c80b54",
      "metadata": {
        "id": "03c80b54",
        "outputId": "ede9075c-f921-4241-a09a-1b23478f7d9f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' 10.'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "convo.run(\"How much is 5+5?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07342f88",
      "metadata": {
        "id": "07342f88",
        "outputId": "4cffe431-329c-4e0d-9ad3-b3cba100d143"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' The captain of the West Indies team that won the first Cricket World Cup in 1975 was Clive Lloyd.'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "convo.run(\"Who was the captain ofthe winning team?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e459d07",
      "metadata": {
        "id": "4e459d07",
        "outputId": "3934a95c-5bfb-4742-946d-e3e7be03cf2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Human: Who won the first cricket world cup?\n",
            "AI:  The first Cricket World Cup was won by the West Indies in 1975.\n",
            "Human: How much is 5+5?\n",
            "AI:  10.\n",
            "Human: Who was the captain ofthe winning team?\n",
            "AI:  The captain of the West Indies team that won the first Cricket World Cup in 1975 was Clive Lloyd.\n"
          ]
        }
      ],
      "source": [
        "print(convo.memory.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feaa3abd",
      "metadata": {
        "id": "feaa3abd"
      },
      "source": [
        "#### ConversationBufferWindowMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "460eb33c",
      "metadata": {
        "id": "460eb33c",
        "outputId": "6e456af5-c577-47d0-b206-bc0c0c751605"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' The first Cricket World Cup was held in 1975 and the winner was the West Indies.'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "memory = ConversationBufferWindowMemory(k=1)\n",
        "\n",
        "convo = ConversationChain(\n",
        "    llm=OpenAI(temperature=0.7),\n",
        "    memory=memory\n",
        ")\n",
        "convo.run(\"Who won the first cricket world cup?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d395beaf",
      "metadata": {
        "id": "d395beaf",
        "outputId": "6d4f9469-f58d-4a34-e932-6707f88f4312"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' 5+5 is 10.'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "convo.run(\"How much is 5+5?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93b24745",
      "metadata": {
        "id": "93b24745",
        "outputId": "af287923-8f55-41a4-a54f-79d45c0adc0d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\" I'm sorry, I don't know.\""
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "convo.run(\"Who was the captain of the winning team?\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gradio"
      ],
      "metadata": {
        "id": "T27vokE_xXL7"
      },
      "id": "T27vokE_xXL7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries- using gradio\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain, SequentialChain\n",
        "from gradio import Interface, inputs, outputs\n",
        "\n",
        "# Setting up the LLM chains\n",
        "llm = OpenAI(temperature=0.7)\n",
        "\n",
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables=['cuisine'],\n",
        "    template=\"I want to open a restaurant for {cuisine} food. Suggest a fancy name for this.\"\n",
        ")\n",
        "\n",
        "name_chain = LLMChain(llm=llm, prompt=prompt_template_name, output_key=\"restaurant_name\")\n",
        "\n",
        "prompt_template_items = PromptTemplate(\n",
        "    input_variables=['restaurant_name'],\n",
        "    template=\"Suggest some menu items for {restaurant_name}.\"\n",
        ")\n",
        "\n",
        "food_items_chain = LLMChain(llm=llm, prompt=prompt_template_items, output_key=\"menu_items\")\n",
        "\n",
        "chain = SequentialChain(\n",
        "    chains=[name_chain, food_items_chain],\n",
        "    input_variables=['cuisine'],\n",
        "    output_variables=['restaurant_name', \"menu_items\"]\n",
        ")\n",
        "\n",
        "# Define the function to be used by Gradio\n",
        "def get_restaurant_details(cuisine):\n",
        "    result = chain({\"cuisine\": cuisine})\n",
        "\n",
        "    # Check if menu_items is a string or a list\n",
        "    if isinstance(result['menu_items'], str):\n",
        "        menu_str = result['menu_items']\n",
        "    else:\n",
        "        # Filter out empty or None values from menu_items before joining\n",
        "        cleaned_menu_items = [item for item in result['menu_items'] if item and item.strip()]\n",
        "        menu_str = \", \".join(cleaned_menu_items)\n",
        "\n",
        "    return result['restaurant_name'], menu_str\n",
        "\n",
        "# Create Gradio interface\n",
        "iface = Interface(\n",
        "    fn=get_restaurant_details,\n",
        "    inputs=inputs.Textbox(label=\"Cuisine\"),\n",
        "    outputs=[\n",
        "        outputs.Textbox(label=\"Restaurant Name\"),\n",
        "        outputs.Textbox(label=\"Menu Items\")\n",
        "    ],\n",
        "    live=True,\n",
        "    layout=\"vertical\",\n",
        "    title = \"Restaraunt Business Bot\"\n",
        ")\n",
        "\n",
        "iface.launch(share=\"True\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 831
        },
        "id": "hZy9lmh1xVqx",
        "outputId": "40dd2f7c-cfbf-472f-9bf9-9fc05a7aeb10"
      },
      "id": "hZy9lmh1xVqx",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-9cc92c3ba75d>:47: GradioDeprecationWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
            "  inputs=inputs.Textbox(label=\"Cuisine\"),\n",
            "<ipython-input-16-9cc92c3ba75d>:47: GradioDeprecationWarning: `optional` parameter is deprecated, and it has no effect\n",
            "  inputs=inputs.Textbox(label=\"Cuisine\"),\n",
            "<ipython-input-16-9cc92c3ba75d>:47: GradioDeprecationWarning: `numeric` parameter is deprecated, and it has no effect\n",
            "  inputs=inputs.Textbox(label=\"Cuisine\"),\n",
            "<ipython-input-16-9cc92c3ba75d>:49: GradioDeprecationWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
            "  outputs.Textbox(label=\"Restaurant Name\"),\n",
            "<ipython-input-16-9cc92c3ba75d>:50: GradioDeprecationWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
            "  outputs.Textbox(label=\"Menu Items\")\n",
            "<ipython-input-16-9cc92c3ba75d>:45: GradioDeprecationWarning: `layout` parameter is deprecated, and it has no effect\n",
            "  iface = Interface(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://d6e692b5975be3f7c7.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d6e692b5975be3f7c7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain"
      ],
      "metadata": {
        "id": "fBrc8jr8DXAE"
      },
      "id": "fBrc8jr8DXAE",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature=0.9)\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=['cuisine'],\n",
        "    template=\"I want to open a restaurant for {cuisine} food. Suggest a fancy name for this.\"\n",
        ")\n",
        "\n",
        "name_chain = LLMChain(llm=llm, prompt=prompt_template, output_key=\"restaurant_name\")\n"
      ],
      "metadata": {
        "id": "j_Ih2DgaG71Q"
      },
      "id": "j_Ih2DgaG71Q",
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature = 0.9)\n",
        "\n",
        "prompt_template_name= PromptTemplate(\n",
        "    input_variables = ['cuisine'],\n",
        "    template = \"i want to open a restaraunt for  {cuisine} food. Suggest a fancy name for this\"\n",
        "                                         )\n",
        "name_chain=LLMChain(llm=llm,prompt=prompt_template_name,output_key=\"restaraunt_name\")\n",
        "\n",
        "llm = OpenAI(temperature = 0.9)\n",
        "prompt_template_items = PromptTemplate(input_variables=['restaraunt_name'],\n",
        "                                      template=\"Suggest me some menu items for {restaraunt_name}.\"\n",
        "                                       )\n",
        "\n",
        "food_items_chain=LLMChain(llm=llm,prompt=prompt_template_items,output_key=\"menu_items\")\n",
        "\n",
        "\n",
        "\n",
        "from langchain.chains import SequentialChain\n",
        "\n",
        "chain = SequentialChain(\n",
        "    chains = [name_chain, food_items_chain],\n",
        "    input_variables =['cuisine',],\n",
        "    output_variables = ['restaraunt_name','menu_items']\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "V_fSDK0TDWQX"
      },
      "id": "V_fSDK0TDWQX",
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain({\"cuisine\": \"French\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZ4-IUHKqHPU",
        "outputId": "30a8971b-be96-415d-f360-b6dde56e0fcb"
      },
      "id": "QZ4-IUHKqHPU",
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cuisine': 'French',\n",
              " 'restaraunt_name': ' restaraunt\\n\\nLe Petit Gourmet Francais',\n",
              " 'menu_items': '\\n\\nAppetizers:\\n- Escargots\\n- Frisée Salad\\n- Steak Tartare\\n- Frog Legs Provençal\\n\\nEntrées:\\n- Coq au Vin\\n- Onion Soup Gratinée\\n- Trout Grenobloise\\n- Bœuf Bourguignon\\n\\nDesserts:\\n- Crème Brûlée\\n- Almond Pear Tart\\n- L’opera Cake\\n- Chocolate Mousse'}"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}